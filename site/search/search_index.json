{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Analityka danych w transporcie", "text": "Kurs adresowany jest do os\u00f3b kt\u00f3re chc\u0105 ropozpocz\u0105\u0107 lub zwi\u0119kszy\u0107 swoje umiej\u0119tno\u015bci w analizie danych. Na kursie om\u00f3wimy przyk\u0142ady z dziedziny transportu i smart city, a pozyskane umiej\u0119tno\u015bci b\u0119d\u0105 s\u0142u\u017cy\u0142y w  ka\u017cdej innej dziedzinie.  <p>W trakcie zaj\u0119\u0107 poznamy nowoczesne, chmurowe \u015brodowisko analityczne Databricks pozwalaj\u0105ce na analiz\u0119 i  wizualizacj\u0119 danych z dowolnych plik\u00f3w, baz danych, \u017cr\u00f3de\u0142 czasu rzeczywistego (streamin danych) oraz us\u0142ug opartych o REST API. </p>"}, {"location": "#przeglad-kursu", "title": "Przegl\u0105d kursu", "text": "<p>Om\u00f3wienie architektury Medalionu i nowoczesnej analizy w  Delta Lake  oraz sposob\u00f3w jej wykorzytania w bie\u017c\u0105cym powiadamianiu, monitorowaniu i raportowaniu. Wprowadzenie do architektury Lakehouse i Delta Lake. Analiza danych z plik\u00f3w, baz danych i API. </p> <p>Praca w interaktywnym \u015brodowisku notatnik\u00f3w. Krok po kroku zapoznanie ze \u015brodowiskiem Databricks oraz poznanie standardu DELTA LAKE.</p> <p>Analizy na danych z istniej\u0105cych system\u00f3w/aplikacji. W tym z systemu detekcji wideo i ruchu pojazd\u00f3w transportu publicznego.  </p> <p> Tworzenie przep\u0142yw\u00f3w (workflows) i zada\u0144 (jobs) oraz ich harmonogramowanie. Efektem ko\u0144cowym b\u0119d\u0105 raporty  zbudowane bezpo\u015brednio na platformie Databricks.</p>"}, {"location": "#dostep-do-materiaow", "title": "\ud83d\udd10 Dost\u0119p do materia\u0142\u00f3w", "text": "<p>Uczestnicy kursu otrzymaj\u0105 dost\u0119p do materia\u0142\u00f3w na pocz\u0105tku zaj\u0119\u0107.</p> <p>\u00a9 2025 Krzysztof Modelewski</p>"}, {"location": "0-WorkshopDescription/course_description/", "title": "Analityka danych w transporcie", "text": ""}, {"location": "0-WorkshopDescription/course_description/#program-szkolenia", "title": "Program szkolenia", "text": ""}, {"location": "0-WorkshopDescription/course_description/#harmonogram-szkolenia", "title": "Harmonogram szkolenia", "text": ""}, {"location": "0-WorkshopDescription/course_description/#dzien-1", "title": "Dzien 1", "text": "<ol> <li>Analityka Self Service 09:00 - 11:00<ul> <li>Definicja i zastosowanie Lakehouse </li> <li>Architektura Medalionowa </li> <li>Data governance</li> <li>Otwarty Format Delta i jego zastosowanie w analityce danych</li> <li>W\u0142a\u015bciwo\u015bci formatu Delta</li> <li>[Lab 1] Dane z KPD GDDKiA</li> </ul> </li> <li>Wprowadzenie do platformy Databricks 11:15 - 14:00<ul> <li>Architektura (Data Plane i Control Plane)</li> <li>Praca z notatnikami</li> </ul> </li> <li>Obiekty Databricks<ul> <li>Klastry obliczeniowe (Compute)</li> <li>Przep\u0142ywy danych (Workflows)</li> <li>Zadania (Jobs)</li> <li>Notatniki (Notebooks)</li> <li>Raporty (Dashboard)</li> </ul> </li> <li>Otoczenie Databricks:<ul> <li>MS Azure (Blob Storage, Key Vault)</li> <li>Amazon AWS (S3)</li> <li>[Lab 2] KPD GDDKiA w Databricks</li> </ul> </li> <li>Wprowadzenie do analizy danych w SQL 14:15 - 16:00</li> <li>Wprowadzenie do SQL<ul> <li>Group by</li> <li>Case When</li> <li>Wyra\u017cenia CTE</li> <li>[Lab 3] Przetwarzanie plik\u00f3w z KPD GDDKiA w SQL.</li> </ul> </li> </ol>"}, {"location": "0-WorkshopDescription/course_description/#dzien-2", "title": "Dzien 2", "text": "<ol> <li>Wprowadzenie analizy danych w j\u0119zyku Python 9:00 - 12:00</li> <li>[Lab 4] Python - typy danych</li> <li>[Lab 4] Python - pobieranie rozk\u0142adu jazdy przez API</li> <li> <p>Wprowadzenie do bilbioteki Pandas</p> </li> <li> <p>Analiza danych 11:15 - 15:00</p> <ul> <li>[Lab 6]</li> <li>[Lab 2] Analiza danych z transportu publicznego (.json/.gtfs)</li> </ul> </li> <li> <p>Przetwarzenie r\u00f3wnoleg\u0142e w Apache Spark 15:00 - 16:00</p> </li> </ol>"}, {"location": "0-WorkshopDescription/course_description/#dzien-3", "title": "Dzien 3", "text": "<ol> <li>Podstawowe transformacje w Pyspark</li> <li>filtrowanie</li> <li>grupowanie</li> <li>praca z r\u00f3\u017cnymi typami plik\u00f3w</li> <li>[lab] Analiza danych w Pyspark</li> <li>Przej\u015bcia pomi\u0119dzy SQL, Pandas i Pyspark</li> <li>Optymalizacja zapisu danych</li> <li>[lab] Projekt ko\u0144cowy</li> </ol>"}, {"location": "0-WorkshopDescription/course_description/#prowadzacy", "title": "Prowadzacy", "text": "<p>Krzysztof Modelewski jest absolwentem Wydzia\u0142u Transportu Politechniki Warszawskiej o specjalno\u015bci Telematyka Transportu. Uczestnik szkole\u0144 w zakresie Inteligentnych System\u00f3w Transportowych, m.in. projektowania sygnalizacji \u015bwietlnych. Autor ksi\u0105\u017cki \u201cInteligentny Transport\u201d. Kierownik budowy \u201eKrajowego Punktu Dost\u0119powego do informacji o warunkach ruchu\u201d https://kpd.gddkia.gov.pl/ . Posiadacz m.in. certyfikat\u00f3w Project Management Professional  i OMG-Certified Expert in BPM (Business Process Management). Obecnie Senior Data Engineer zajmuj\u0105cy si\u0119 zaawansowan\u0105 analityk\u0105 danych z wykorzystaniem  technologii chmurowych m.in. Databricks, Microsoft Fabric i Snowflake.</p>"}, {"location": "0-WorkshopDescription/python_overview/", "title": "Python overview", "text": ""}, {"location": "0-WorkshopDescription/sql_spark_sql/", "title": "Python overview", "text": ""}, {"location": "1-Przyklady/1_1_CreatingClusters/", "title": "Tworzenie klastra", "text": "<p>1-\tNavigate to the Compute tab in the left side bar.</p> <p>2-\tUnder All-purpose compute tab, click Create compute.</p> <p>3-\tOn top, click on the default name to change it. Name your cluster as Demo Cluster</p> <p>4-\tSelect Single node cluster</p> <p>5-\tSelect the Databricks runtime version 13.3 LTS (Long Term Support)</p> <p>6-\tUncheck the option for the Use Photon Acceleration</p> <p>7-\tSelect a Node type of 4 cores</p> <p>8-\tSet the auto termination of the cluster to 30 minutes</p> <p>9-\tLastly, click Create compute.</p>"}, {"location": "1-Przyklady/1_1_CreatingClusters/#creating-clusters", "title": "Creating Clusters\u00b6", "text": ""}, {"location": "1-Przyklady/1_1_CreatingClusters/#creating-a-demo-cluster", "title": "Creating a Demo Cluster\u00b6", "text": "<p>Create a cluster with the following configurations:</p> Setting Instructions Cluster name Demo Cluster Cluster mode Signle node Runtime version Select the Databricks runtime version 13.3 LTS Photon Acceleration Uncheck the option Node type 4 cores Auto termination 30 minutes"}, {"location": "1-Przyklady/1_1_analiza_danych/", "title": "Analiza csv", "text": "In\u00a0[1]: Copied! <pre>print(1)\n</pre> print(1) <pre>1\n</pre> In\u00a0[2]: Copied! <pre>Hello\n</pre> Hello <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 Hello\n\nNameError: name 'Hello' is not defined</pre>"}, {"location": "1-Przyklady/1_1_analiza_danych/#reading-csv-a-plot-the-results", "title": "Reading csv a plot the results\u00b6", "text": ""}, {"location": "1-Przyklady/1_1_databricks/", "title": "Platforma analityczna", "text": ""}, {"location": "1-Przyklady/1_1_databricks/#databricks", "title": "Databricks", "text": "<p>Unified analytics platform built on Lakehouse Architecture with default format: Delta Delta official</p>"}, {"location": "1-Przyklady/1_1_databricks/#storage", "title": "Storage", "text": "<p>Structured and semi-structured data. Delta format</p>"}, {"location": "1-Przyklady/1_1_databricks/#compute", "title": "Compute", "text": "<p>Spark engine</p> <p>Heavy workloads and growing business needs requires another aproach to computing. One mainframe computers are expensive and diffcult to upgrade. Below example of Apache spark architecture.</p> <p></p> <p>Partitioning (Huge table - partition - files). Subdividing data by column. Do not use partition for high  data cardinality use. load_date, country. Problem malych plikow. Avoid column with high skewness or null values.</p> id lane_no vehicle_count sys_load_date 1 1 12 2025-03-24 partitionBy<pre><code>df.write.format(\"delta\").partitionBy(\"sys_load_date\").saveAsTable(\"traffic_monitoring\")\n</code></pre> ACID <p>Atomicity (all transactions complete with success of complete failure). Consistency (state of the data is the same for simultaneous operations).  Isolation (how simultaneous operations potentially confflic with one another - optimistic concurenncy control). Durability (commitet changes are permanent). </p> <p>Thanks to: write serializable isolation and optimistic concurenncy control</p>"}, {"location": "1-Przyklady/1_1_databricks/#parallel-computing", "title": "Parallel computing", "text": ""}, {"location": "1-Przyklady/1_1_databricks/#use-cases", "title": "Use Cases", "text": "<ol> <li>Reporting (Power BI)</li> </ol> <ol> <li>Real time management (bike sharing stations with ticketing system)</li> </ol> <p> https://community.databricks.com/t5/technical-blog/real-time-vehicle-fleet-analytics-with-databricks-delta-live/ba-p/91422?lightbox-message-images-91422=11415i9EEB3BC3960C04C9</p> <ol> <li> <p>Combine multiple data</p> </li> <li> <p>Data sharing </p> </li> <li>internal</li> <li>external</li> </ol>"}, {"location": "1-Przyklady/1_1_lakehouse/", "title": "Lakehouse", "text": ""}, {"location": "1-Przyklady/1_1_lakehouse/#definicje", "title": "Definicje", "text": "<ol> <li>Data Lake - skalowalna przestrze\u0144 kt\u00f3ra przechowauje dane surowe. Data lake bazuje na systemach plik\u00f3w i umieszczana jest na klastrach (HDFS, Cloud e.g. ADSL Gen 2)</li> <li>Data Warehouse - przestrze\u0144 do sk\u0142adowania danych ustrukturyzowanych (tabularycznych). Zwykle umieszczona na jednej maszynie, a co za tym idzie ma\u0142o skalowalna.</li> <li>Lakehouse - or data lakehouse - Oferuje najlepsze cechy Data Lake i Data Warehouse.</li> <li>Delta lake - storage layer for lakehouse based on open standard Delta Lake (parquet data files  with file-based transatcion log for ACID transactions)</li> <li>Databricks - Unified platform built on Lakehouse Architecture with default format: Delta Delta official</li> </ol>"}, {"location": "1-Przyklady/1_1_lakehouse/#architektura-medalionu", "title": "Architektura Medalionu", "text": "<p>To wzorzec projektu s\u0142u\u017c\u0105cy do organizowania danych w Lakehouse. Sk\u0142ada si\u0119 z trzech warstw: Bronze =&gt; Silver =&gt; Gold</p> <p></p> Bronze Silver Gold Raw data, where data comes from external systems. Cleansed and preprocessed data \"Project - specific\" (Product). Usually for reporting purposes (fact and dimensions tables) Managed by Data Engineers and Administrators with no access to other uses Source data for self-service analytics  Table 1: Opis tabel w architekturze medalionu <p>Data Lake (storage, external storage + warehouse (plus sql endpoint))</p>"}, {"location": "1-Przyklady/1_1_lakehouse/#przykady-wykorzystania-lakehouse", "title": "Przyk\u0142ady wykorzystania Lakehouse", "text": "<ol> <li>Raportowanie dzienne</li> </ol> <ol> <li>Zarz\u0105dzanie w czasie rzeczywistym (bike sharing stations with ticketing system)</li> </ol> <p> https://community.databricks.com/t5/technical-blog/real-time-vehicle-fleet-analytics-with-databricks-delta-live/ba-p/91422?lightbox-message-images-91422=11415i9EEB3BC3960C04C9</p> <ol> <li> <p>Demokratyzacja danych Demokratyzacja danych to proces umo\u017cliwiaj\u0105cy dost\u0119p do danych wszystkim pracownikom organizacji,  niezale\u017cnie od ich umiej\u0119tno\u015bci technicznych. W Lakehouse, dane s\u0105 przechowywane w formacie Delta, co u\u0142atwia ich udost\u0119pnianie i analiz\u0119.</p> </li> <li> <p>Udost\u0119pnienie danych</p> </li> <li>Delta sharing</li> <li>Clean rooms</li> </ol>"}, {"location": "1-Przyklady/assets/python_data_types1/", "title": "Python data types1", "text": "In\u00a0[\u00a0]: Copied! <pre>### Hello\n</pre>  ### Hello In\u00a0[1]: Copied! <pre>print(1)\n</pre> print(1) <pre>1\n</pre> In\u00a0[2]: Copied! <pre>Hello\n</pre> Hello <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 Hello\n\nNameError: name 'Hello' is not defined</pre>"}, {"location": "2-Databricks/db_platform_architecture/", "title": "1.Architecture", "text": ""}, {"location": "2-Databricks/db_platform_architecture/#platform-description", "title": "Platform Description", "text": "<p>Compute  Storage Administration</p>"}, {"location": "2-Databricks/db_platform_architecture/#noteboook-environments", "title": "Noteboook environments", "text": ""}, {"location": "2-Databricks/python_data_types1/", "title": "Lab Python Data Types", "text": "In\u00a0[\u00a0]: Copied! <pre>### Hello\n</pre>  ### Hello In\u00a0[1]: Copied! <pre>print(1)\n</pre> print(1) <pre>1\n</pre> In\u00a0[2]: Copied! <pre>Hello\n</pre> Hello <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 Hello\n\nNameError: name 'Hello' is not defined</pre>"}, {"location": "index_assets/presentation_1/slide_1/", "title": "Medallion Architecture", "text": "<ul> <li>Bullet point 1</li> <li>Bullet point 2</li> </ul>"}, {"location": "index_assets/presentation_1/slide_2/", "title": "Slide Title", "text": "<ul> <li>Bullet point 1</li> <li>Bullet point 2</li> </ul>"}, {"location": "index_assets/presentation_1/slide_2/#subheading", "title": "Subheading", "text": "<p>More content here...</p>"}]}