{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "O kursie", "text": ""}, {"location": "#analityka-danych-w-transporcie", "title": "Analityka danych w transporcie", "text": "Kurs adresowany jest do os\u00f3b kt\u00f3re chc\u0105 ropozpocz\u0105\u0107 lub zwi\u0119kszy\u0107 swoje umiej\u0119tno\u015bci w analizie danych. Na kursie om\u00f3wimy przyk\u0142ady z dziedziny transportu i smart city, a pozyskane umiej\u0119tno\u015bci b\u0119d\u0105 s\u0142u\u017cy\u0142y w  ka\u017cdej innej dziedzinie.  <p>W trakcie zaj\u0119\u0107 poznamy nowoczesne, chmurowe \u015brodowisko analityczne Databricks pozwalaj\u0105ce na analiz\u0119 i  wizualizacj\u0119 danych z dowolnych plik\u00f3w, baz danych, \u017cr\u00f3de\u0142 czasu rzeczywistego (streamin danych) oraz us\u0142ug opartych o REST API. </p>"}, {"location": "#przeglad-kursu", "title": "Przegl\u0105d kursu", "text": "<p>Om\u00f3wienie architektury Medalionu i nowoczesnej analizy w  Delta Lake  oraz sposob\u00f3w jej wykorzytania w bie\u017c\u0105cym powiadamianiu, monitorowaniu i raportowaniu. Wprowadzenie do architektury Lakehouse i Delta Lake. Analiza danych z plik\u00f3w, baz danych i API. </p> <p>Praca w interaktywnym \u015brodowisku notatnik\u00f3w. Krok po kroku zapoznanie ze \u015brodowiskiem Databricks oraz poznanie standardu DELTA LAKE.</p> <p>Analizy na danych z istniej\u0105cych system\u00f3w/aplikacji. W tym z systemu detekcji wideo i ruchu pojazd\u00f3w transportu publicznego.  </p> <p> Tworzenie przep\u0142yw\u00f3w (workflows) i zada\u0144 (jobs) oraz ich harmonogramowanie. Efektem ko\u0144cowym b\u0119d\u0105 raporty  zbudowane bezpo\u015brednio na platformie Databricks.</p>"}, {"location": "#dostep-do-materiaow", "title": "\ud83d\udd10 Dost\u0119p do materia\u0142\u00f3w", "text": "<p>Uczestnicy kursu otrzymaj\u0105 dost\u0119p do materia\u0142\u00f3w na pocz\u0105tku zaj\u0119\u0107.</p> <p>\u00a9 2025 Krzysztof Modelewski</p>"}, {"location": "0-WorkshopDescription/course_description/", "title": "Plan Kursu", "text": ""}, {"location": "0-WorkshopDescription/course_description/#analityka-danych-w-transporcie", "title": "Analityka danych w transporcie", "text": ""}, {"location": "0-WorkshopDescription/course_description/#program-szkolenia", "title": "Program szkolenia", "text": ""}, {"location": "0-WorkshopDescription/course_description/#harmonogram-szkolenia", "title": "Harmonogram szkolenia", "text": ""}, {"location": "0-WorkshopDescription/course_description/#dzien-1", "title": "Dzien 1", "text": "<ol> <li>Analityka Self Service 09:00 - 11:00<ul> <li>Definicja i zastosowanie Lakehouse </li> <li>Architektura Medalionowa </li> <li>Data governance</li> <li>Otwarto\u015b\u0107 danych</li> <li>Przegl\u0105d platform Databricks, Snowflake, Microsoft Fabric</li> <li>Otwarty Format Delta i jego zastosowanie w analityce danych</li> <li>W\u0142a\u015bciwo\u015bci formatu Delta</li> <li>[Lab 1] Standard Delta Lake</li> </ul> </li> <li>[Lab 2] Wprowadzenie do platformy Databricks - KPD GDDKiA 11:15 - 14:00<ul> <li>Infrastruktura jako kod (terraform)</li> <li>Architektura (Data Plane i Control Plane)</li> <li>Praca z notatnikami</li> <li>Obiekty Databricks<ul> <li>Klastry obliczeniowe (Compute)</li> <li>Przep\u0142ywy danych (Workflows)</li> <li>Zadania (Jobs i Pipelines)</li> <li>Notatniki (Notebooks)</li> <li>Raporty (Dashboard)</li> </ul> </li> </ul> </li> <li>Otoczenie Databricks:<ul> <li>MS Azure (Blob Storage, Key Vault)</li> <li>Amazon AWS (S3)</li> <li>Pobranie archiwalnych danych KPD GDDKiA do Databricks</li> </ul> </li> <li>Wprowadzenie do analizy danych w SQL 14:15 - 16:00<ul> <li>Group by</li> <li>Case When</li> <li>Common Table Expressions (CTE)</li> <li>Joins</li> <li>Window Functions</li> <li>[Lab 4] Przetwarzanie plik\u00f3w z KPD GDDKiA w SQL.</li> </ul> </li> </ol>"}, {"location": "0-WorkshopDescription/course_description/#dzien-2", "title": "Dzien 2", "text": "<ol> <li>Wprowadzenie analizy danych w j\u0119zyku Python 9:00 - 12:00<ul> <li>Wprowadzenie do python <ul> <li>typy danych</li> <li>klasy i funkcje</li> </ul> </li> <li>Przetwarzanie r\u00f3wnoleg\u0142e w Spark<ul> <li>architektura Spark</li> <li>wyzwania przetwarzania w Big Data</li> </ul> </li> </ul> </li> <li>Wprowadzenie do biblioteki Pandas i Pyspark 13:00 - 16:00<ul> <li>podstawowe transformacje w Pyspark</li> <li>praca z r\u00f3\u017cnymi typami plik\u00f3w</li> <li>przej\u015bcie pomi\u0119dzy SQL, Pandas i Pyspark</li> <li>Optymalizacja zapisu danych</li> </ul> </li> </ol>"}, {"location": "0-WorkshopDescription/course_description/#dzien-3", "title": "Dzien 3", "text": "<ol> <li>Analiza danych w transporcie publicznym (*.json)<ul> <li>zebranie danych z API (otwarte publicznie dane)</li> <li>utworzenie modelu danych</li> <li>utworzenie przep\u0142ywu danych</li> </ul> </li> <li>Wizualizacja danych</li> <li>Udost\u0119pnianie danych (Delta Sharing, Clean Room)</li> </ol>"}, {"location": "0-WorkshopDescription/course_description/#prowadzacy", "title": "Prowadzacy", "text": "<p>Krzysztof Modelewski jest absolwentem Wydzia\u0142u Transportu Politechniki Warszawskiej o specjalno\u015bci Telematyka Transportu. Uczestnik szkole\u0144 w zakresie Inteligentnych System\u00f3w Transportowych, m.in. projektowania sygnalizacji \u015bwietlnych. Autor ksi\u0105\u017cki \u201cInteligentny Transport\u201d. Kierownik budowy \u201eKrajowego Punktu Dost\u0119powego do informacji o warunkach ruchu\u201d https://kpd.gddkia.gov.pl/ . Posiadacz m.in. certyfikat\u00f3w Project Management Professional  i OMG-Certified Expert in BPM (Business Process Management). Obecnie Senior Data Engineer zajmuj\u0105cy si\u0119 zaawansowan\u0105 analityk\u0105 danych z wykorzystaniem  technologii chmurowych m.in. Databricks, Microsoft Fabric i Snowflake.</p>"}, {"location": "1-PrzykladoweWarsztaty/1_1_CreatingClusters/", "title": "1 1 CreatingClusters", "text": "<p>1-\tNavigate to the Compute tab in the left side bar.</p> <p>2-\tUnder All-purpose compute tab, click Create compute.</p> <p>3-\tOn top, click on the default name to change it. Name your cluster as Demo Cluster</p> <p>4-\tSelect Single node cluster</p> <p>5-\tSelect the Databricks runtime version 13.3 LTS (Long Term Support)</p> <p>6-\tUncheck the option for the Use Photon Acceleration</p> <p>7-\tSelect a Node type of 4 cores</p> <p>8-\tSet the auto termination of the cluster to 30 minutes</p> <p>9-\tLastly, click Create compute.</p>"}, {"location": "1-PrzykladoweWarsztaty/1_1_CreatingClusters/#tworzenie-klastra-obliczeniowego", "title": "Tworzenie klastra obliczeniowego\u00b6", "text": ""}, {"location": "1-PrzykladoweWarsztaty/1_1_CreatingClusters/#tworzeie-klastra", "title": "Tworzeie klastra\u00b6", "text": "<p>Stw\u00f3rz klaster z nast\u0119puj\u0105cymi ustawieniami:</p> Parametr Nazwa Nazwa klastra Demo Cluster Typ klastra Signle node Runtime version Select the Databricks runtime version 13.3 LTS Photon Acceleration Uncheck the option Node type 4 cores Auto termination 30 minutes"}, {"location": "1-PrzykladoweWarsztaty/1_1_processing_files/", "title": "File processing", "text": ""}, {"location": "1-PrzykladoweWarsztaty/1_1_processing_files/#file-sources", "title": "File sources", "text": ""}, {"location": "1-PrzykladoweWarsztaty/1_1_processing_files/#spark-sql", "title": "SPARK SQL", "text": ""}, {"location": "1-PrzykladoweWarsztaty/1_1_processing_files/#1-parquet-files", "title": "1. Parquet files", "text": "SQL<pre><code>--INFO: this not relates to SQL ENDPOINT (querying warehouse)\n\n--parquet from abfss\nSELECT * FROM parquet.`abfss://container@storageaccount.dfs.core.windows.net/xxxx`\n--parquet from abfss (internal storage - Databricks File System like HDFS)\nSELECT * FROM parquet.`dbfs:/user/hive/warehouse/my_table/file_test.snappy.parquet`\n</code></pre>"}, {"location": "1-PrzykladoweWarsztaty/1_1_processing_files/#1-csv-files", "title": "1. CSV files", "text": "SQL<pre><code>--INFO: this not relates to SQL ENDPOINT (querying warehouse)\n\n--parquet from abfss\nSELECT * FROM read_files(`abfss://container@storageaccount.dfs.core.windows.net/xxxx`,\n    format =&gt; 'csv',\n    sep =&gt; ';',\n    header =&gt; true,\n    mode =&gt; 'FAILFAST'\n\n\n--parquet from abfss (internal storage - Databricks File System like HDFS)\nSELECT * FROM parquet.`dbfs:/user/hive/warehouse/my_table/file_test.snappy.parquet`\n</code></pre>"}, {"location": "1-PrzykladoweWarsztaty/example_workshops/", "title": "Lakehouse", "text": ""}, {"location": "1-PrzykladoweWarsztaty/example_workshops/#definicje", "title": "Definicje", "text": "<ol> <li>Data Lake - skalowalna przestrze\u0144 kt\u00f3ra przechowauje dane surowe. Data lake bazuje na systemach plik\u00f3w i umieszczana jest na klastrach (HDFS, Cloud e.g. ADSL Gen 2)</li> <li>Data Warehouse - przestrze\u0144 do sk\u0142adowania danych ustrukturyzowanych (tabularycznych). Zwykle umieszczona na jednej maszynie, a co za tym idzie ma\u0142o skalowalna.</li> <li>Lakehouse - or data lakehouse - Oferuje najlepsze cechy Data Lake i Data Warehouse.</li> <li>Delta lake - storage layer for lakehouse based on open standard Delta Lake (parquet data files  with file-based transatcion log for ACID transactions)</li> <li>Databricks - platforma danych zbudowana na bazie Lakehouse z domy\u015blnym formatem danych Delta Delta official</li> </ol>"}, {"location": "1-PrzykladoweWarsztaty/example_workshops/#architektura-medalionu", "title": "Architektura Medalionu", "text": "<p>To wzorzec projektu s\u0142u\u017c\u0105cy do organizowania danych w Lakehouse. Sk\u0142ada si\u0119 z trzech warstw: Bronze =&gt; Silver =&gt; Gold</p> <p></p> Bronze Silver Gold Dane surowe, pochodz\u0105ce najcz\u0119\u015bciej z innych system\u00f3w Wst\u0119pnie wyczyszczone i przeprocesowane dane Najcz\u0119\u015bciej tabele fakt\u00f3w i wymiar\u00f3w pod raportowanie Zarz\u0105dzanie przez In\u017cynier\u00f3w Danych Dane \u017ar\u00f3d\u0142owe do analityki  Table 1: Opis tabel w architekturze medalionu"}, {"location": "1-PrzykladoweWarsztaty/spark_example/", "title": "Apache Spark", "text": ""}, {"location": "1-PrzykladoweWarsztaty/spark_example/#apache-spark-engine", "title": "Apache spark engine", "text": "<p>Obliczenia na du\u017cych zbiorach danych wymagaj\u0105 innego, r\u00f3wnoleg\u0142ego podej\u015bcia do przetwarzania.  Powszechnie stosowanym systemem przetwarzania r\u00f3wnoleg\u0142ego jest Apache Spark. Architektura spark opiera si\u0119 na modelu master-slave. G\u0142\u00f3wny w\u0119ze\u0142 (driver) zarz\u0105dza zadaniami i koordynuje prac\u0119 w\u0119z\u0142\u00f3w roboczych (workers). Ka\u017cdy w\u0119ze\u0142 roboczy sk\u0142ada si\u0119 z wielu wykonawc\u00f3w (executors), kt\u00f3re wykonuj\u0105 zadania r\u00f3wnolegle.</p> <p></p> <p>Partycjonowanie danych - jednym z najisottniejszych zagadnie\u0144 w przetwarzaniu rozproszonym jest zminimalizownaie liczby przyczytanych danych (dane s\u0105 fizycznie sk\u0142adowanie w plikach). Jednym z najwcze\u015bniejszych sposob\u00f3w na optymalizacj\u0119 zapyta\u0144 jest partycjonowanie danych. Partycjonowanie to podzia\u0142 du\u017cej tabeli na mniejsze, bardziej zarz\u0105dzalne cz\u0119\u015bci, zwane partycjami. Partycje s\u0105 tworzone na podstawie warto\u015bci w jednej lub wi\u0119cej kolumn tabeli. Dzi\u0119ki temu, gdy zapytanie jest wykonywane, tylko odpowiednie partycje s\u0105 odczytywane, co znacznie zmniejsza ilo\u015b\u0107 przetwarzanych danych i poprawia wydajno\u015b\u0107 zapyta\u0144.</p> <p>Dzisiejsze platformy przetwarzania bazuj\u0105ce na standardzie DELTA automatycznie zarz\u0105dzaj\u0105 partycjonowaniem danych, czego przyk\u0142adem jest np. Liquid Clustering. Zapewnia on opytamaln\u0105 wielko\u015b\u0107 plik\u00f3w i zarz\u0105dzanie nimi.</p> id lane_no vehicle_count sys_load_date 1 1 12 2025-03-24 partitionBy<pre><code>df.write.format(\"delta\").partitionBy(\"sys_load_date\").saveAsTable(\"traffic_monitoring\")\n</code></pre>"}, {"location": "1-Przyklady/1_1_CreatingClusters/", "title": "1 1 CreatingClusters", "text": "<p>1-\tNavigate to the Compute tab in the left side bar.</p> <p>2-\tUnder All-purpose compute tab, click Create compute.</p> <p>3-\tOn top, click on the default name to change it. Name your cluster as Demo Cluster</p> <p>4-\tSelect Single node cluster</p> <p>5-\tSelect the Databricks runtime version 13.3 LTS (Long Term Support)</p> <p>6-\tUncheck the option for the Use Photon Acceleration</p> <p>7-\tSelect a Node type of 4 cores</p> <p>8-\tSet the auto termination of the cluster to 30 minutes</p> <p>9-\tLastly, click Create compute.</p>"}, {"location": "1-Przyklady/1_1_CreatingClusters/#creating-clusters", "title": "Creating Clusters\u00b6", "text": ""}, {"location": "1-Przyklady/1_1_CreatingClusters/#creating-a-demo-cluster", "title": "Creating a Demo Cluster\u00b6", "text": "<p>Create a cluster with the following configurations:</p> Setting Instructions Cluster name Demo Cluster Cluster mode Signle node Runtime version Select the Databricks runtime version 13.3 LTS Photon Acceleration Uncheck the option Node type 4 cores Auto termination 30 minutes"}, {"location": "1-Przyklady/1_1_databricks/", "title": "1 1 databricks", "text": ""}, {"location": "1-Przyklady/1_1_databricks/#databricks", "title": "Databricks", "text": "<p>Unified analytics platform built on Lakehouse Architecture with default format: Delta Delta official</p>"}, {"location": "1-Przyklady/1_1_databricks/#storage", "title": "Storage", "text": "<p>Structured and semi-structured data. Delta format</p>"}, {"location": "1-Przyklady/1_1_databricks/#compute", "title": "Compute", "text": "<p>Spark engine</p> <p>Heavy workloads and growing business needs requires another aproach to computing. One mainframe computers are expensive and diffcult to upgrade. Below example of Apache spark architecture.</p> <p></p> <p>Partitioning (Huge table - partition - files). Subdividing data by column. Do not use partition for high  data cardinality use. load_date, country. Problem malych plikow. Avoid column with high skewness or null values.</p> id lane_no vehicle_count sys_load_date 1 1 12 2025-03-24 partitionBy<pre><code>df.write.format(\"delta\").partitionBy(\"sys_load_date\").saveAsTable(\"traffic_monitoring\")\n</code></pre> ACID <p>Atomicity (all transactions complete with success of complete failure). Consistency (state of the data is the same for simultaneous operations).  Isolation (how simultaneous operations potentially confflic with one another - optimistic concurenncy control). Durability (commitet changes are permanent). </p> <p>Thanks to: write serializable isolation and optimistic concurenncy control</p>"}, {"location": "1-Przyklady/1_1_databricks/#parallel-computing", "title": "Parallel computing", "text": ""}, {"location": "1-Przyklady/1_1_databricks/#use-cases", "title": "Use Cases", "text": "<ol> <li>Reporting (Power BI)</li> </ol> <ol> <li>Real time management (bike sharing stations with ticketing system)</li> </ol> <p> https://community.databricks.com/t5/technical-blog/real-time-vehicle-fleet-analytics-with-databricks-delta-live/ba-p/91422?lightbox-message-images-91422=11415i9EEB3BC3960C04C9</p> <ol> <li> <p>Combine multiple data</p> </li> <li> <p>Data sharing </p> </li> <li>internal</li> <li>external</li> </ol>"}, {"location": "1-Przyklady/1_1_lakehouse/", "title": "1 1 lakehouse", "text": ""}, {"location": "1-Przyklady/1_1_lakehouse/#definicje", "title": "Definicje", "text": "<ol> <li>Data Lake - skalowalna przestrze\u0144 kt\u00f3ra przechowauje dane surowe. Data lake bazuje na systemach plik\u00f3w i umieszczana jest na klastrach (HDFS, Cloud e.g. ADSL Gen 2)</li> <li>Data Warehouse - przestrze\u0144 do sk\u0142adowania danych ustrukturyzowanych (tabularycznych). Zwykle umieszczona na jednej maszynie, a co za tym idzie ma\u0142o skalowalna.</li> <li>Lakehouse - or data lakehouse - Oferuje najlepsze cechy Data Lake i Data Warehouse.</li> <li>Delta lake - storage layer for lakehouse based on open standard Delta Lake (parquet data files  with file-based transatcion log for ACID transactions)</li> <li>Databricks - Unified platform built on Lakehouse Architecture with default format: Delta Delta official</li> </ol>"}, {"location": "1-Przyklady/1_1_lakehouse/#architektura-medalionu", "title": "Architektura Medalionu", "text": "<p>To wzorzec projektu s\u0142u\u017c\u0105cy do organizowania danych w Lakehouse. Sk\u0142ada si\u0119 z trzech warstw: Bronze =&gt; Silver =&gt; Gold</p> <p></p> Bronze Silver Gold Raw data, where data comes from external systems. Cleansed and preprocessed data \"Project - specific\" (Product). Usually for reporting purposes (fact and dimensions tables) Managed by Data Engineers and Administrators with no access to other uses Source data for self-service analytics  Table 1: Opis tabel w architekturze medalionu <p>Data Lake (storage, external storage + warehouse (plus sql endpoint))</p>"}, {"location": "1-Przyklady/1_1_lakehouse/#przykady-wykorzystania-lakehouse", "title": "Przyk\u0142ady wykorzystania Lakehouse", "text": "<ol> <li>Raportowanie dzienne</li> </ol> <ol> <li>Zarz\u0105dzanie w czasie rzeczywistym (bike sharing stations with ticketing system)</li> </ol> <p> https://community.databricks.com/t5/technical-blog/real-time-vehicle-fleet-analytics-with-databricks-delta-live/ba-p/91422?lightbox-message-images-91422=11415i9EEB3BC3960C04C9</p> <ol> <li> <p>Demokratyzacja danych Demokratyzacja danych to proces umo\u017cliwiaj\u0105cy dost\u0119p do danych wszystkim pracownikom organizacji,  niezale\u017cnie od ich umiej\u0119tno\u015bci technicznych. W Lakehouse, dane s\u0105 przechowywane w formacie Delta, co u\u0142atwia ich udost\u0119pnianie i analiz\u0119.</p> </li> <li> <p>Udost\u0119pnienie danych</p> </li> <li>Delta sharing</li> <li>Clean rooms</li> </ol>"}, {"location": "1-Przyklady/assets/python_data_types1/", "title": "Python data types1", "text": "In\u00a0[\u00a0]: Copied! <pre>### Hello\n</pre>  ### Hello In\u00a0[1]: Copied! <pre>print(1)\n</pre> print(1) <pre>1\n</pre> In\u00a0[2]: Copied! <pre>Hello\n</pre> Hello <pre>---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 Hello\n\nNameError: name 'Hello' is not defined</pre>"}, {"location": "2-Databricks/python_data_types1/", "title": "Python data types1", "text": "In\u00a0[\u00a0]: Copied! <pre>### Hello\n</pre>  ### Hello In\u00a0[1]: Copied! <pre>print(1)\n</pre> print(1) <pre>1\n</pre> In\u00a0[2]: Copied! <pre>Hello\n</pre> Hello <pre>---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 Hello\n\nNameError: name 'Hello' is not defined</pre>"}, {"location": "4-Spark/window_functions/", "title": "Window functions", "text": ""}, {"location": "4-Spark/window_functions/#partycjonowanie-w-sql", "title": "Partycjonowanie w SQL", "text": "<p>Partycjonowanie, inaczej grupowanie, pozwala po\u0142\u0105czy\u0107 wyniki zapytan na r\u00f3\u017cnych pozioach og\u00f3lno\u015bci. Do partycjonowania s\u0142u\u017cy klauzula Over. Over pozwala na wywo\u0142anie funkcji rankinowych, okienkowych i analitycznych.</p> Text Only<pre><code>    - window, row_number, rank, dense_rank, percent_rank\n</code></pre>"}, {"location": "functions_list/data_ingestion/", "title": "Function list", "text": ""}, {"location": "functions_list/data_ingestion/#function-list", "title": "Function list", "text": "<ol> <li> <p>read_files()</p> </li> <li> <p>sk\u0142adnia: read_files(path [, option_key =&gt; option_value ] [...])</p> </li> <li>opis: wczytuje pliki z lokalizacji chmurowych. Rozszerzenia plik\u00f3w: JSON, CSV, XML, TEXT, BINARYFILE, PARQUET, AVRO, and ORC. </li> <li>mode =&gt; 'FAILFAST' - przerywa proces wczytywania wyrzucaj\u0105c b\u0142\u0105d</li> <li>mode =&gt; 'PERMISSIVE' - wstawia null</li> <li>mode =&gt; 'DROPMALFORMED' - ignoruje z\u0142e wiersze</li> <li>komentarz: _metadata jest struct i zawiera file_path, file_name...</li> <li> <p>dokumentacja: https://docs.databricks.com/aws/en/sql/language-manual/functions/read_files</p> </li> <li> <p>.selectExpr(\"*\", \"_metadata as source_metadata\") \\</p> </li> </ol>"}, {"location": "index_assets/presentation_1/slide_1/", "title": "Medallion Architecture", "text": ""}, {"location": "index_assets/presentation_1/slide_1/#medallion-architecture", "title": "Medallion Architecture", "text": "<ul> <li>Bullet point 1</li> <li>Bullet point 2</li> </ul>"}, {"location": "index_assets/presentation_1/slide_2/", "title": "Slide Title", "text": ""}, {"location": "index_assets/presentation_1/slide_2/#slide-title", "title": "Slide Title", "text": "<ul> <li>Bullet point 1</li> <li>Bullet point 2</li> </ul>"}, {"location": "index_assets/presentation_1/slide_2/#subheading", "title": "Subheading", "text": "<p>More content here...</p>"}]}